{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Introduction to Reddit Data\n",
    "\n",
    "In this lab, we'll cover:\n",
    "- What reddit data look like\n",
    "- Several ways to summarize the conversation's tone\n",
    "- Evaluation of data over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "- Data files with Reddit comments are publicly available many places online, including torrents, google's BigQuery, and several data hosting websites. UM keeps a full copy in our Advanced Research Computing resources.\n",
    "- Reddit is one of the biggest sites on the internet. \n",
    "    - It has over 3 billion comments, and the data take up several TB of disk space (`1TB = 1024 GB`)! \n",
    "    - This makes working with the data difficult.\n",
    "    - For simplicity, we went ahead and used some big data tools like `pyspark` and `hadoop` to go through all the comments and select out smaller sets to work with in this lab. \n",
    "- Let's start by looking at just the comments from the subreddit community for the University of Michigan\n",
    "    - This file is only 34 MB: a more managable size!\n",
    "    - The `shape` property tells us that there are 66 thousand rows (comments) and 51 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments = pd.read_csv('data/merged/uofm.tsv', sep='\\t')\n",
    "um_comments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little cleanup for our data\n",
    "- Don't worry about this code, just run it and scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(df):\n",
    "    df['date'] = pd.to_datetime(df.created_utc, unit='s')\n",
    "    drop_cols = ['approved_at_utc', 'approved_by', 'archived', \n",
    "                 'author_cakeday', 'author_flair_css_class', \n",
    "                 'author_flair_text', 'banned_at_utc', 'banned_by',\n",
    "                 'can_gild', 'can_mod_post', 'collapsed', \n",
    "                 'collapsed_reason', 'created_utc', 'distinguished', \n",
    "                 'downs', 'likes', 'removal_reason', \n",
    "                 'report_reasons', 'retrieved_on', 'saved',\n",
    "                 'score_hidden', 'stickied',  'subreddit_id', 'ups']\n",
    "    df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "um_comments = clean_up(um_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What information do we have about each comment?\n",
    "- We have a lot! Here are some of the most interesting columns:\n",
    "    - `body` the text of the comment\n",
    "    - `author` the username of the person who posted it\n",
    "    - `created_utc` when the comment was made\n",
    "    - `subreddit` which community a comment is from. Here, they're all from `r/uofm`\n",
    "    - Several scores from the [Perspective API](https://www.perspectiveapi.com/). In this project, Google and Jigsaw teamed up to build automatic systems for finding bad comments. We used their program to score these comments already, and the scores are saved in the file.\n",
    "        - `ATTACK_ON_COMMENTER` the probability that this comment is a personal attack on another commenter\n",
    "        - `INCOHERENT` whether the comment seems to make sense\n",
    "        - `INFLAMMATORY` how inflammatory the comment is\n",
    "        - `LIKELY_TO_REJECT` the liklihood that New York Times comment editors would reject  the comment if it was posted on their site\n",
    "        - `OBSCENE` whether the comment is obscene\n",
    "        - `TOXICITY` whether the comment is 'toxic' for community discussion\n",
    "    - `politeness` scores, computed by the [Stanford NLP group's software](https://www.cs.cornell.edu/~cristian/Politeness.html).\n",
    "    - `sentiment` (how positive or negative a comment is), computed by the [VADER program in NLTK](http://www.nltk.org/_modules/nltk/sentiment/vader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example comments\n",
    "- This randomly selects one of the comments and shows the text. Run it multiple times to see different randomly chosen comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(um_comments.sample(1).body.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a feel for our data\n",
    "- One of the first things to do with any data is plot it. We want to get a feel for what's in it. \n",
    "- Some of the scores are normally distributed, like sentiment and politeness. \n",
    "    - What might this mean?\n",
    "    - Why might there be a spike of comments with exactly 0 (totally neutral) sentiment?\n",
    "- The distributions of other scores, like personal attacks and obscenity are very skewed. \n",
    "    - Most comments are nice (low scores), but a few are not (high scores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.sentiment.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.politeness.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.ATTACK_ON_COMMENTER.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.OBSCENE.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated scores aren't perfect.\n",
    "- Here we have a function that will show us examples of some comments that score highest or lowest in one of the measures. \n",
    "- The function picks a comment at random, so run it more than once and you'll see different comments.\n",
    "- Note that the scores don't always seem right. For example, sometimes a comment that scored high in `ATTACK_ON_COMMENTER` isn't actually a personal attack.\n",
    "    - The scores were made by some of the most advanced software for this in the world, and they're still not perfect. This reminds us just how hard it is for computers to understand human language.\n",
    "- Still, most of the scores seem about right. And, as we know from statistics, we can still make inferences about average scores even when there are some errors in our measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(data, column, where='high'):\n",
    "    #pick whether to use high or low scoring comments\n",
    "    if where == 'high':\n",
    "        asc = False\n",
    "    else:\n",
    "        asc = True\n",
    "    #Select the 100 most extreme comments in this column\n",
    "    df = data.sort_values(by=column, ascending=asc).head(100)\n",
    "    #pick one at random and print the text of it\n",
    "    print(df.sample(1).body.iloc[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_example(data=um_comments, column='sentiment', where='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_example(data=um_comments, column='politeness', where='low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_example(data=um_comments, \n",
    "            column='ATTACK_ON_COMMENTER', where='high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing trends over time\n",
    "- In this lab, we're not just interested in individual comments, but in the community (in this case, a subreddit forum) and how it changes over time. \n",
    "- To study this, we're going to be using the `groupby` and `resample` functions in pandas. They're two slightly different functions that do the same basic thing:\n",
    "    - Take all of our comments and put them into groups (in our case, one group for each month).\n",
    "    - Summarize each group (e.g. by telling us how many comments are in it or what their average score is).\n",
    "- Once we have summaries for each group, we can plot them on a graph where the X axis is time. Take a look at the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the comments by month\n",
    "monthly = um_comments.resample('M', on='date')\n",
    "\n",
    "#count the number of comments in ach group\n",
    "total_comments = monthly.body.count()\n",
    "\n",
    "#count the number of unique comment authors in each group\n",
    "active_users = monthly.author.nunique()\n",
    "\n",
    "#show first few months\n",
    "active_users.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_comments.plot(title='Number of comments posted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all months in a graph\n",
    "active_users.plot(title='Number of active users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for patterns\n",
    "- Do you notice a pattern in the number of comments or active users over time?\n",
    "    - It is a little messy, but it seems like there are less people posting comments in the middle of each year (summer time). Why might that be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
