{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count pejorative nouns\n",
    "\n",
    "Based on the paper:\n",
    "- Palmer, Alexis, Melissa Robinson, and Kristy Philips. 2017. “Illegal Is Not a Noun: Linguistic Form for Detection of Pejorative Nominalizations.” Pp. 91–100 in *Proceedings of the First Workshop on Abusive Language Online.* Vancouver. Retrieved February 17, 2018 (http://www.aclweb.org/anthology/W17-3014).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import ipyparallel\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "c = ipyparallel.Client()\n",
    "view = c.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaks the dataframe into chunks that can be processed separately\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "#counts pejorative nouns\n",
    "def process_df(df):\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    \n",
    "    keep = lambda pos: \"NN\" in pos[:2]\n",
    "    \n",
    "    def get_pej_noun(txt):\n",
    "        output = 0\n",
    "        txt = str(txt)\n",
    "        \n",
    "        #words the paper identified as usually pejorative when used as nouns\n",
    "        bad_nouns = ['female', 'gay', 'illegal', 'poor']\n",
    "        \n",
    "        #skip empty comments\n",
    "        if len(txt) == 0:\n",
    "            pass\n",
    "        elif txt == 'nan':\n",
    "            pass\n",
    "        elif txt == '[deleted]':\n",
    "            pass\n",
    "        elif txt == '[removed]':\n",
    "            pass\n",
    "        else:\n",
    "            #tokenize sentences, then words within them\n",
    "            words = [nltk.word_tokenize(s) for s in nltk.sent_tokenize(txt)]\n",
    "            #tag all words for part of speech\n",
    "            tagged = [word for sent in nltk.pos_tag_sents(words) for word in sent]\n",
    "            #flatten list and keep only the nouns\n",
    "            nouns = [word for (word, pos) in tagged if keep(pos)]\n",
    "            \n",
    "            #iterate over all nouns\n",
    "            for n in nouns:\n",
    "                #check each to see if it contains one of the bad nouns\n",
    "                for b in bad_nouns:\n",
    "                    if b in n:\n",
    "                        output += 1 #count total bad nouns\n",
    "            \n",
    "        return output\n",
    "\n",
    "    df['pej_nouns'] = df.body.apply(get_pej_noun)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already finished demsocialist.tsv\n",
      "already finished puppies.tsv\n",
      "already finished republicans.tsv\n",
      "already finished CatGifs.tsv\n",
      "already finished cats.tsv\n",
      "already finished OhioStateFootball.tsv\n",
      "already finished GreenParty.tsv\n",
      "already finished MichiganWolverines.tsv\n",
      "already finished msu.tsv\n",
      "already finished StartledCats.tsv\n",
      "already finished uofm.tsv\n",
      "already finished PussyPass.tsv\n",
      "already finished CatsStandingUp.tsv\n",
      "already finished dogpictures.tsv\n",
      "already finished OSU.tsv\n",
      "already finished communism.tsv\n",
      "already finished TrollXChromosomes_short.tsv\n",
      "already finished democrats.tsv\n",
      "already finished Liberal.tsv\n",
      "adding dogs_short.tsv\n",
      "adding socialism_short.tsv\n",
      "adding TheRedPill_short.tsv\n",
      "adding TwoXChromosomes_short.tsv\n",
      "adding MensRights_short.tsv\n",
      "adding Libertarian_short.tsv\n",
      "adding progressive.tsv\n",
      "adding pussypassdenied.tsv\n",
      "adding FULLCOMMUNISM.tsv\n",
      "adding Republican.tsv\n",
      "adding Dogtraining.tsv\n",
      "adding NeutralPolitics.tsv\n",
      "adding socialism.tsv\n",
      "adding dogs.tsv\n",
      "adding TheRedPill.tsv\n",
      "adding TrollXChromosomes.tsv\n",
      "adding MensRights.tsv\n",
      "adding Libertarian.tsv\n",
      "already finished TwoXChromosomes.tsv\n"
     ]
    }
   ],
   "source": [
    "def get_files_by_file_size(dirname, reverse=False):\n",
    "    \"\"\" Return list of file paths in directory sorted by file size \"\"\"\n",
    "\n",
    "    l = len(dirname)\n",
    "    # Get list of files\n",
    "    filepaths = []\n",
    "    for basename in os.listdir(dirname):\n",
    "        filename = os.path.join(dirname, basename)\n",
    "        if os.path.isfile(filename):\n",
    "            filepaths.append(filename)\n",
    "    for i in range(len(filepaths)):\n",
    "        filepaths[i] = (filepaths[i], os.path.getsize(filepaths[i]))\n",
    "    filepaths.sort(key=lambda filename: filename[1], reverse=reverse)\n",
    "    for i in range(len(filepaths)):\n",
    "        filepaths[i] = filepaths[i][0][l+1:]\n",
    "\n",
    "    return filepaths\n",
    "           \n",
    "def get_jobs():\n",
    "    jobs = []\n",
    "    \n",
    "    files = get_files_by_file_size('../../sampled', reverse=False)\n",
    "    #if 'TwoXChromosomes.tsv' in files:\n",
    "    #    files.remove('TwoXChromosomes.tsv')\n",
    "    #    files.insert(0, 'TwoXChromosomes.tsv')\n",
    "    done = os.listdir('../data/pej_nouns/')\n",
    "    \n",
    "    for f in files:\n",
    "        if f.endswith('tsv'):\n",
    "            if f in done:\n",
    "                print('already finished', f)\n",
    "                pass\n",
    "            else:\n",
    "                print('adding', f)\n",
    "                tmp = {}\n",
    "                tmp['file'] = '/home/jwlock/research/reddit/sampled/'+f\n",
    "                tmp['subreddit'] = f[:-4]\n",
    "                jobs.append(tmp)\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "jobs = get_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064/3991 tasks finished after  490 s"
     ]
    }
   ],
   "source": [
    "while len(jobs)>0:\n",
    "    j = jobs[0]\n",
    "    print('Working on', j['subreddit'])\n",
    "    df = pd.read_csv(j['file'], sep='\\t', usecols=['id', 'body'])\n",
    "    chunks = chunker(df, 1000)\n",
    "    result = view.map_async(process_df, chunks)\n",
    "    result.wait_interactive()\n",
    "    df = pd.concat(result)\n",
    "    df.to_csv('../data/pej_nouns/'+j['subreddit']+'.tsv', sep='\\t', index=False)\n",
    "    jobs = get_jobs()\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
