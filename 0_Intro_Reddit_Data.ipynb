{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Introduction to Reddit Data\n",
    "\n",
    "In this lab, we'll cover:\n",
    "- What reddit data look like\n",
    "- Several ways to summarize the conversation's tone\n",
    "- Evaluation of data over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "- Data files with Reddit comments are publicly available many places online, including torrents, google's BigQuery, and several data hosting websites. UM keeps a full copy in our Advanced Research Computing resources.\n",
    "- Reddit is one of the biggest sites on the internet. \n",
    "    - It has over 3 billion comments, and the data take up several TB of disk space (`1TB = 1024 GB`)! \n",
    "    - This makes working with the data difficult.\n",
    "    - For simplicity, we went ahead and used some big data tools like `pyspark` and `hadoop` to go through all the comments and select out smaller sets to work with in this lab. \n",
    "- Let's start by looking at just the comments from the subreddit community for the University of Michigan\n",
    "    - This file is only 34 MB: a more managable size!\n",
    "    - The `shape` property tells us that there are 66 thousand rows (comments) and 51 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments = pd.read_csv('data/merged/uofm.tsv', sep='\\t')\n",
    "um_comments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What information do we have about each comment?\n",
    "- We have a lot! Here are some of the most interesting columns:\n",
    "    - `body` the text of the comment\n",
    "    - `author` the username of the person who posted it\n",
    "    - `created_utc` when the comment was made\n",
    "    - `subreddit` which community a comment is from. Here, they're all from `r/uofm`\n",
    "    - Several scores from the [Perspective API](https://www.perspectiveapi.com/). In this project, Google and Jigsaw teamed up to build automatic systems for finding bad comments. We used their program to score these comments already.\n",
    "        - `ATTACK_ON_COMMENTER` the probability that this comment is a personal attack on another commenter\n",
    "        - `INCOHERENT` whether the comment seems to make sense\n",
    "        - `INFLAMMATORY` how inflammatory the comment is\n",
    "        - `LIKELY_TO_REJECT` the liklihood that New York Times comment editors would reject  the comment if it was posted on their site\n",
    "        - `OBSCENE` whether the comment is obscene\n",
    "        - `TOXICITY` whether the comment is 'toxic' for community discussion\n",
    "    - `politeness` scores, computed by the [Stanford NLP group's software](https://www.cs.cornell.edu/~cristian/Politeness.html).\n",
    "    - `sentiment` (how positive or negative a comment is), computed by the [VADER program in NLTK](http://www.nltk.org/_modules/nltk/sentiment/vader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a feel for our data\n",
    "- Some of the scores are normally distributed, like sentiment and politeness. \n",
    "    - What might this mean?\n",
    "    - Why might there be a spike of comments with exactly 0 (totally neutral) sentiment?\n",
    "- The distributions of other scores, like personal attacks and obscenity are very skewed. \n",
    "    - Most comments are nice (low scores), but a few are not (high scores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.sentiment.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.politeness.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.ATTACK_ON_COMMENTER.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_comments.OBSCENE.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated scores aren't perfect.\n",
    "- Here we have a function that will show us examples of some comments that score highest or lowest in one of the measures. \n",
    "- The function is a little random, so run it more than once and you'll see different comments.\n",
    "- Note that the scores don't always seem right. For example, sometimes a comment that scored high in `ATTACK_ON_COMMENTER` isn't actually a personal attack.\n",
    "    - The scores were made by some of the most advanced software for this in the world, and they're still not perfect. This reminds us just how hard it is for computers to understand human language.\n",
    "- Still, most of the scores seem about right. And, as we know from statistics, we can still make inferences about average scores even when there are some errors in our measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(data, column, where='high'):\n",
    "    #pick whether to use high or low scoring comments\n",
    "    if where == 'high':\n",
    "        asc = False\n",
    "    else:\n",
    "        asc = True\n",
    "    #Select the 100 most extreme comments in this column\n",
    "    df = data.sort_values(by=column, ascending=asc).head(100)\n",
    "    #pick one at random and print the text of it\n",
    "    print(df.sample(1).body.iloc[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_example(data=um_comments, column='sentiment', where='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_example(data=um_comments, column='politeness', where='low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_example(data=um_comments, \n",
    "            column='ATTACK_ON_COMMENTER', where='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
