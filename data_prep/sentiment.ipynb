{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import ipyparallel\n",
    "import os\n",
    "import sys\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "c = ipyparallel.Client()\n",
    "view = c.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(txt):\n",
    "    sents = sent_tokenize(txt)\n",
    "    model = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "    for s in sents:\n",
    "        results.append(model.polarity_scores(s)['compound'])\n",
    "    return np.mean(results)\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Here is some sample text. THe text makes me happy. Much like my amazing cat. But not like that garbage windows.\"\n",
    "get_sentiment(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    import pandas as pd\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    import numpy as np\n",
    "    \n",
    "    def get_sentiment(txt):\n",
    "        sents = sent_tokenize(str(txt))\n",
    "        model = SentimentIntensityAnalyzer()\n",
    "        results = []\n",
    "        for s in sents:\n",
    "            results.append(model.polarity_scores(s)['compound'])\n",
    "        return np.mean(results)\n",
    "\n",
    "    df['sentiment'] = df.body.apply(get_sentiment)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_by_file_size(dirname, reverse=False):\n",
    "    \"\"\" Return list of file paths in directory sorted by file size \"\"\"\n",
    "\n",
    "    l = len(dirname)\n",
    "    # Get list of files\n",
    "    filepaths = []\n",
    "    for basename in os.listdir(dirname):\n",
    "        filename = os.path.join(dirname, basename)\n",
    "        if os.path.isfile(filename):\n",
    "            filepaths.append(filename)\n",
    "    for i in range(len(filepaths)):\n",
    "        filepaths[i] = (filepaths[i], os.path.getsize(filepaths[i]))\n",
    "    filepaths.sort(key=lambda filename: filename[1], reverse=reverse)\n",
    "    for i in range(len(filepaths)):\n",
    "        filepaths[i] = filepaths[i][0][l+1:]\n",
    "\n",
    "    return filepaths\n",
    "           \n",
    "def get_jobs():\n",
    "    jobs = []\n",
    "    \n",
    "    files = get_files_by_file_size('../sampled', reverse=False)\n",
    "    if 'TwoXChromosomes.tsv' in files:\n",
    "        files.remove('TwoXChromosomes.tsv')\n",
    "        files.insert(0, 'TwoXChromosomes.tsv')\n",
    "    done = os.listdir('data/sentiment/')\n",
    "    \n",
    "    for f in files:\n",
    "        if f.endswith('tsv'):\n",
    "            if f in done:\n",
    "                print('already finished', f)\n",
    "                pass\n",
    "            else:\n",
    "                print('adding', f)\n",
    "                tmp = {}\n",
    "                tmp['file'] = '/home/jwlock/research/reddit/sampled/'+f\n",
    "                tmp['subreddit'] = f[:-4]\n",
    "                jobs.append(tmp)\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "jobs = get_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(jobs)>0:\n",
    "    j = jobs[0]\n",
    "    print('Working on', j['subreddit'])\n",
    "    df = pd.read_csv(j['file'], sep='\\t')\n",
    "    chunks = chunker(df, 500)\n",
    "    result = view.map_async(process_df, chunks)\n",
    "    result.wait_interactive()\n",
    "    df = pd.concat(result)\n",
    "    df.to_csv('data/sentiment/'+j['subreddit']+'.tsv', sep='\\t', index=False)\n",
    "    jobs = get_jobs()\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
